{"name":"Spark XGboost","tagline":"","body":"# SparkXGBoost \r\n\r\nSparkXGBoost aims to implement the [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) tree algorithm in [XGBoost](https://github.com/dmlc/xgboost/) on the [Apache Spark](http://spark.apache.org) platform. `SparkXGBoost` is distributed under Apache License 2.0. \r\n\r\n[![Build Status](https://travis-ci.org/rotationsymmetry/SparkXGBoost.svg?branch=master)](https://travis-ci.org/rotationsymmetry/SparkXGBoost) \r\n[![codecov.io](https://codecov.io/github/rotationsymmetry/SparkXGBoost/coverage.svg?branch=master)](https://codecov.io/github/rotationsymmetry/SparkXGBoost?branch=master)\r\n\r\n## Introduction to Gradient Boosting Trees\r\nThe XGBoost team have a fantastic [introduction](http://xgboost.readthedocs.org/en/latest/model.html) to gradient boosting trees, which inspires `SparkXGBoost`. \r\n\r\n## Features\r\nSparkXGBoost version 0.1 supports supervised learning using the gradient boosting tree with second order approximation of arbitrary user-defined loss function. The following `Loss` classes are provided with the package: \r\n\r\n* `SquareLoss` for linear (normal) regression\r\n* `LogisticLoss` for binary classification\r\n* `PoissonLoss` for Poisson regression of count data\r\n\r\nSparkXGBoost includes following approach to avoid overfitting\r\n\r\n* L2 regularization term on node\r\n* L1 regularization term on node\r\n* Stochastic gradient boosting (similar to Bagging)\r\n* Feature sub sampling for learning nodes\r\n\r\nSparkXGBoost is capable of processing multiple learning nodes in the one pass of the training data to improve efficiency. \r\n\r\n## Components\r\nThere are three major components:\r\n\r\n`SparkXGBoost` is the learner class. Its constructor takes an instance from the `Loss` class that defines the loss in gradient boosting.  After fitting the tree ensembles with the training data, it will produce the model as an instance from `SparkXGBoost` class. \r\n\r\n``` scala\r\nclass SparkXGBoost(val loss: Loss){\r\n  def fit(dataset: DataFrame): SparkXGBoostModel\r\n}\r\n```\r\n\r\n`SparkXGBoostModel` contains the trained tree ensemble and is capable to making predictions for the instances.\r\n\r\n``` scala\r\nclass SparkXGBoostModel {\r\n  // Predict label given the feature of a single instance\r\n  def predict(features: Vector): Double\r\n  // Provide prediction for the entire dataset\r\n  def transform(dataset: DataFrame): SparkXGBoostModel\r\n}\r\n```\r\n\r\nThe abstract class `Loss` defines the contract for user-defined loss functions. \r\n\r\n``` scala\r\nabstract class Loss{\r\n  // The 1st derivative\r\n  def diff1(label: Double, f: Double): Double\r\n  // The 2nd derivative \r\n  def diff2(label: Double, f: Double): Double\r\n  // Generate prediction from the score suggested by the tree ensemble\r\n  def toPrediction(score: Double): Double\r\n  // obtain bias \r\n  def getInitialBias(input: RDD[LabeledPoint]): Double\r\n}\r\n```\r\n\r\n## Use SparkXGBoost in Your Project\r\n\r\nFirstly, clone the project from github\r\n\r\n``` bash\r\ngit clone https://github.com/rotationsymmetry/SparkXGBoost.git\r\n```\r\n\r\nSecondly, compile and package the jar using [sbt](http://www.scala-sbt.org)\r\n\r\n``` bash \r\ncd SparkXGBoost\r\nsbt package clean package\r\n```\r\n\r\nYou should be able to find the jar file in `target/target/scala-2.10/sparkxgboost_2.10-0.1.jar`\r\n\r\nLastly, load it in your Spark project\r\n\r\n* If you are using spark-shell, you can type in\r\n\r\n``` bash\r\n./spark-shell --jars path/to/sparkxgboost_2.10-0.1.jar\r\n```\r\n\r\n* If you are building Spark application with sbt, you can put the jar file into the `lib` folder next to `src`. Then sbt should be able to put SparkXGBoost in your class path.\r\n\r\n## Example\r\n\r\nBelow is an example running SparkXGBoost. `trainingData` is a `DataFrame` with the labels stored in a column named \"label\" and the feature vectors stored in a column name \"features\".  Similarly, `testData` is `DataFrame` with the feature vectors stored in a column name \"features\". \r\n\r\nPleaes note that the feature vectors have to been indexed before feeding to the `SparkXGBoost` and `SparkXGBoostModel` to ensure the categorical variables are correctly encoded with metadata.\r\n\r\nIn SparkXGBoost 0.1, all categorical variables are assumed to be ordered. Unordered categorical variables can be used for training after being coded with [OneHotEncoder](http://spark.apache.org/docs/latest/ml-features.html#onehotencoder). \r\n\r\n``` scala\r\n  val featureIndexer = new VectorIndexer()\r\n    .setInputCol(\"features\")\r\n    .setOutputCol(\"indexedFeatures\")\r\n    .setMaxCategories(2)\r\n    .fit(trainingData)\r\n\r\n  val sXGBoost = new SparkXGBoost(new SquareLoss)\r\n    .setFeaturesCol(\"indexedFeatures\")\r\n    .setMaxDepth(1)\r\n    .setNumTrees(1)\r\n  val sXGBoostModel = sXGBoost.fit(\r\n    featureIndexer.transform(trainingData))\r\n\r\n  val predictionData = sXGBoostModel.transform(\r\n    featureIndexer.transform(testData))\r\n```\r\n\r\n## Parameters\r\nThe following parameters can be specified by the setters in `SXGBoost` .\r\n\r\n* labelCol[default=\"label\"]\r\n\t* the name of the label column of the `DataFrame`\r\n\t* String\r\n* featuresCol[default=\"features\"]\r\n\t* the name of the feature column of the `DataFrame`\r\n\t* String\r\n* numTrees[default=1]\r\n\t* number of trees to be grown in the boosting algorithm.\r\n\t* Int, range: [1, ∞]\r\n* maxDepth [default=5]\r\n\t* maximum depth of a tree. A tree with one root and two leaves is considered to have depth = 1.\r\n\t* Int, range: [1,∞]\r\n* lambda [default=0]\r\n\t* L2 regularization term on weights. \r\n\t* Double, range: [0, ∞]\r\n* alpha [default=0]\r\n\t* L1 regularization term on weights. \r\n\t* Double, range: [0, ∞]\r\n* gamma [default=0]\r\n\t* minimum loss reduction required to make a further partition on a leaf node of the tree. \r\n\t* Double, range: [0, ∞]\r\n* minInstanceWeight [default=1]\r\n\t* minimum weight (aka, number of data instance) required to make a further partition on a leaf node of the tree. \r\n\t* Double, range: [0, ∞]\r\n* sampleRatio [default=1.0]\r\n    * sample ratio of rows in bagging\r\n    * Double, range(0, 1]\r\n* featureSubsampleRatio [default=1.0]\r\n\t* subsample ratio of columns when constructing each tree.\r\n\t* Double, range: (0, 1]\r\n* maxConcurrentNodes[default=50]\r\n\t* maximal number of nodes to be process in one pass of the training data.\r\n\t* Int, [1, ∞]\r\n* maxBins [default=32]\r\n    * maximal number of bins for continuous variables.\r\n    * Int, [2, ∞]\r\n    \r\nThe following parameters can be specified by the setters in `SXGBoostModel` .\r\n\r\n* predictionCol[default=\"prediction\"]\r\n\t* the name of the prediction column of the `DataFrame`\r\n\t* String\r\n* featuresCol[default=\"features\"]\r\n\t* the name of the feature column of the  `DataFrame`\r\n\t* String\r\n\r\n## Roadmap\r\nI have following tentative roadmap for the upcoming releases:\r\n\r\n0.2\r\n\r\n* Support step size\r\n\r\n0.3\r\n\r\n* Post-pruning\r\n\r\n0.4\r\n\r\n* Automatically determine the maximal number of current nodes by memory management\r\n\r\n0.5\r\n\r\n* Multi-class classification\r\n\r\n0.6 \r\n\r\n* Unordered categorical variables\r\n\r\n## Bugs and Improvements\r\n \r\nMany thanks for testing SparkXGBoost! \r\n\r\nYou can file bug report or provide suggestions using [github issues](https://github.com/rotationsymmetry/SparkXGBoost/issues). \r\n\r\nIf you would like to improve the codebase, please don't hesitate to submit a pull request. \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}